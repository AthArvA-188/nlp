{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used Pytorch for execution\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('/data.tsv',\n",
    "                  sep='\\t', on_bad_lines='skip', usecols=['star_rating','review_headline', 'review_body'],\n",
    "                  memory_map=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                    review_headline  \\\n",
       "0           5                                         Five Stars   \n",
       "1           5  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2           5                      but I am sure I will like it.   \n",
       "3           1  and the shredder was dirty and the bin was par...   \n",
       "4           4                                         Four Stars   \n",
       "\n",
       "                                         review_body  \n",
       "0                                     Great product.  \n",
       "1  What's to say about this commodity item except...  \n",
       "2    Haven't used yet, but I am sure I will like it.  \n",
       "3  Although this was labeled as &#34;new&#34; the...  \n",
       "4                    Gorgeous colors and easy to use  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body star_rating\n",
       "0                                     Great product.           5\n",
       "1  What's to say about this commodity item except...           5\n",
       "2    Haven't used yet, but I am sure I will like it.           5\n",
       "3  Although this was labeled as &#34;new&#34; the...           1\n",
       "4                    Gorgeous colors and easy to use           4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_req = data.loc[:, ['review_body', 'star_rating']]\n",
    "data_req.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  1.,  4.,  2.,  3., nan])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_req['star_rating']= pd.to_numeric(data['star_rating'], errors='coerce')\n",
    "data_req['star_rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 1., 4., 2., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_req.dropna(subset=[\"star_rating\"], inplace=True)\n",
    "data_req['star_rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req = data_req.dropna(subset=['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_395084/ipykernel_4126049/2739175969.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_data = data_req.groupby('star_rating').apply(lambda x: x.sample(n=50000, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The photo is deceiving - makes it look like a ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst labels ever! I purchased these labels to...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product broke in a very short time.  It a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The printer head is malfunctioning since the i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When this item shipped to me I was very excite...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0  The photo is deceiving - makes it look like a ...          1.0\n",
       "1  Worst labels ever! I purchased these labels to...          1.0\n",
       "2  This product broke in a very short time.  It a...          1.0\n",
       "3  The printer head is malfunctioning since the i...          1.0\n",
       "4  When this item shipped to me I was very excite...          1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data = data_req.groupby('star_rating').apply(lambda x: x.sample(n=50000, random_state=42)).reset_index(drop=True)\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star_rating\n",
      "1.0    50000\n",
      "2.0    50000\n",
      "3.0    50000\n",
      "4.0    50000\n",
      "5.0    50000\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "2    100000\n",
      "1    100000\n",
      "3     50000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "balanced_data['sentiment'] = balanced_data['star_rating'].apply(lambda x: 1 if x >= 4 else (2 if x <= 2 else 3))\n",
    "print(balanced_data['star_rating'].value_counts())\n",
    "print(balanced_data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding short forms\n",
    "\n",
    "contractions_dict = { 'arent': 'are not', 'wont': 'will not', 'cant': 'can not', 'dont': 'do not', 'shant' : \"shall not\",\n",
    "                     'wouldnt': 'would not', 'couldnt': 'could not', 'shouldnt': 'should not', 'isnt':'is not', 'im':'i am', 'mustnt': 'must not',\n",
    "                     'didnt': 'did not', 'doesnt': 'does not',\n",
    "                     'theyre': \"they are\"}\n",
    "#character with diacritics\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c)!='Mn')\n",
    "#Using the contraction function\n",
    "def expand_contractions(text):\n",
    "    if isinstance(text, str):\n",
    "        for word in contractions_dict:\n",
    "            text = text.replace(word, contractions_dict[word])\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "def cleaned_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "\n",
    "    #URLs\n",
    "    text = re.sub(r\"\\bhttps?:\\/\\/\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(\n",
    "        r\"[a-zA-Z0-9_\\-\\.]+@[a-zA-Z0-9_\\-\\.]+\\.[a-zA-Z]{2,5}\", \" \", text\n",
    "    )\n",
    "    # Remove HTML tags with empty string\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = expand_contractions(text)\n",
    "    # removes all non-alphabetical characters\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]+\", \"\", text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "final_clean = np.vectorize(cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data[\"review_body\"] =balanced_data[\"review_body\"].apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_stpwds = set(stopwords.words('english'))\n",
    "#stop words removal and lemmatization\n",
    "pattern = re.compile(r'\\b('+r'|'.join(req_stpwds)+r')\\b\\s*')\n",
    "\n",
    "def lem_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words= [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = pattern.sub('', text)\n",
    "    text = lem_text(text)\n",
    "    return text\n",
    "preprocess_text_vect = np.vectorize(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data[\"review_body\"]= balanced_data[\"review_body\"].apply(preprocess_text_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photo deceiving make look like set pen fact on...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst label ever purchased label try reading r...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product broke short ti ame also poor job getti...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>printer head malfunctioning since installation...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>item shipped excited outside great quality loo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating  sentiment\n",
       "0  photo deceiving make look like set pen fact on...          1.0          2\n",
       "1  worst label ever purchased label try reading r...          1.0          2\n",
       "2  product broke short ti ame also poor job getti...          1.0          2\n",
       "3  printer head malfunctioning since installation...          1.0          2\n",
       "4  item shipped excited outside great quality loo...          1.0          2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews - Training set: (80000, 1000), Testing set: (20000, 1000)\n",
      "Negative reviews - Training set: (80000, 1000), Testing set: (20000, 1000)\n",
      "Neutral reviews - Training set: (40000, 1000), Testing set: (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, max_features=1000)\n",
    "\n",
    "# Filter positive and negative reviews\n",
    "pos_reviews = balanced_data[balanced_data['sentiment'] == 1]\n",
    "neg_reviews = balanced_data[balanced_data['sentiment'] == 2]\n",
    "neu_reviews = balanced_data[balanced_data['sentiment'] == 3]\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(balanced_data['review_body'])\n",
    "\n",
    "# Transform each subset\n",
    "pos_tfidf = tfidf_vectorizer.transform(pos_reviews['review_body'])\n",
    "neg_tfidf = tfidf_vectorizer.transform(neg_reviews['review_body'])\n",
    "neu_tfidf = tfidf_vectorizer.transform(neu_reviews['review_body'])\n",
    "\n",
    "# Split positive reviews\n",
    "pos_x_train, pos_x_test = train_test_split(pos_tfidf, test_size=0.2, random_state=3)\n",
    "# Split negative reviews\n",
    "neg_x_train, neg_x_test = train_test_split(neg_tfidf, test_size=0.2, random_state=3)\n",
    "# Split neutral reviews\n",
    "neu_x_train, neu_x_test = train_test_split(neu_tfidf, test_size=0.2, random_state=3)\n",
    "\n",
    "print(f\"Positive reviews - Training set: {pos_x_train.shape}, Testing set: {pos_x_test.shape}\")\n",
    "print(f\"Negative reviews - Training set: {neg_x_train.shape}, Testing set: {neg_x_test.shape}\")\n",
    "print(f\"Neutral reviews - Training set: {neu_x_train.shape}, Testing set: {neu_x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'car'\t1.00\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'car'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 dimensions of 'king': [ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477 -0.03613281\n",
      "  0.11181641 -0.19824219  0.05126953  0.36328125]\n"
     ]
    }
   ],
   "source": [
    "word_vector = wv[\"king\"]  # Example: get the word embedding for \"king\"\n",
    "print(\"First 10 dimensions of 'king':\", word_vector[:10])  # Show a part of the vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674735069275), ('princess', 0.5902431011199951), ('crown_prince', 0.549946129322052), ('prince', 0.5377321243286133), ('kings', 0.5236843824386597), ('Queen_Consort', 0.5235944390296936), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411403656006)]\n"
     ]
    }
   ],
   "source": [
    "result = wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567486\n"
     ]
    }
   ],
   "source": [
    "similarity = wv.similarity(\"excellent\", \"outstanding\")\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('south_carolina', 0.6567358374595642), ('arizona', 0.6312313675880432), ('nevada', 0.6222927570343018), ('alabama', 0.6215481162071228), ('utah', 0.6204033493995667)]\n"
     ]
    }
   ],
   "source": [
    "similar_words =wv.most_similar(\"california\")\n",
    "print(similar_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sister', 0.8103213906288147), ('daughter', 0.7646753191947937), ('mother', 0.7524207830429077), ('son', 0.7238258123397827), ('niece', 0.7215942144393921), ('husband', 0.7141482830047607), ('father', 0.7066071629524231), ('aunt', 0.6844728589057922), ('cousin', 0.6844366192817688), ('eldest_daughter', 0.6790661215782166)]\n",
      "[('Windows_Mobile', 0.6286555528640747), ('Windows_Vista', 0.6119047999382019), ('Windows', 0.6092869639396667), ('Windows_Phone', 0.603961169719696), ('WP7', 0.603832483291626), ('Internet_Explorer', 0.5924732685089111), ('MIcrosoft', 0.5920181274414062), ('Windows_Phone_7', 0.5891107320785522), ('Mircosoft', 0.5824356079101562), ('Microsoft_NSDQ_MSFT', 0.5787074565887451)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=[\"brother\",\"woman\"], negative=[\"man\"]))\n",
    "print(wv.most_similar(positive=[\"Microsoft\", \"iPhone\"], negative=[\"Apple\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\tminivan\tOne of these words is not in the vocabulary.\n",
      "car\tbicycle\t0.44\n",
      "car\tairplane\t0.64\n",
      "car\tcar\t1.00\n",
      "car\tcommunism\tOne of these words is not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = [\n",
    "    simple_preprocess(str(doc))\n",
    "    for doc in balanced_data[\"review_body\"]\n",
    "    if isinstance(doc, str)\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=300,\n",
    "    window=11,\n",
    "    min_count=10\n",
    ")\n",
    "\n",
    "# Check semantic similarities for the same pairs\n",
    "for w1, w2 in pairs:\n",
    "    if w1 in w2v_model.wv.key_to_index and w2 in w2v_model.wv.key_to_index:\n",
    "        print(f\"{w1}\\t{w2}\\t{w2v_model.wv.similarity(w1, w2):.2f}\")\n",
    "    else:\n",
    "        print(f\"{w1}\\t{w2}\\tOne of these words is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words for ['brother', 'woman'] - ['man']: [('mfc', 0.5985559821128845), ('dw', 0.5790482759475708), ('hl', 0.5758002400398254), ('tn', 0.575276792049408), ('mfcjdw', 0.5372292399406433), ('mfcdw', 0.5367903709411621), ('hldw', 0.5294637680053711), ('hldn', 0.5283377766609192), ('mfcjw', 0.5197377800941467), ('xerox', 0.5122743248939514)]\n",
      "Most similar words for ['king', 'woman'] - ['man']: [('softcover', 0.45056256651878357), ('comfortably', 0.4200192987918854), ('lifestyle', 0.39905422925949097), ('men', 0.39489373564720154), ('sturdiness', 0.39269891381263733), ('allpurpose', 0.3918309807777405), ('salad', 0.38464289903640747), ('popular', 0.38433322310447693), ('variant', 0.3833228051662445), ('capless', 0.3808940052986145)]\n",
      "The similarity between 'excellent' and 'outstanding' using our trained model: 0.8006961345672607\n",
      "0.49567968\n"
     ]
    }
   ],
   "source": [
    "# Check most similar words based on analogy\n",
    "similar_result1 = w2v_model.wv.most_similar(positive=[\"brother\", \"woman\"], negative=[\"man\"])\n",
    "print(\"Most similar words for ['brother', 'woman'] - ['man']:\", similar_result1)\n",
    "\n",
    "similar_result2 = w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
    "print(\"Most similar words for ['king', 'woman'] - ['man']:\", similar_result2)\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity = w2v_model.wv.similarity(\"excellent\", \"outstanding\")\n",
    "print(f\"The similarity between 'excellent' and 'outstanding' using our trained model: {similarity}\")\n",
    "similarity2 = w2v_model.wv.similarity(\"table\" , \"chair\")\n",
    "print(similarity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our model performs better as we are feeding only the relevent data.\n",
    "# in the case of most similar word using semantic similarities the pretrained model performs better.\n",
    "# for comparing excellent and outstanding our model(self trained) gives better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x_train, pos_x_test = train_test_split(pos_reviews['review_body'], test_size=0.2, random_state=3)\n",
    "neg_x_train, neg_x_test = train_test_split(neg_reviews['review_body'], test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative reviews for binary classification (Neu reviews can be excluded here)\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])]  # Exclude neutral (3)\n",
    "\n",
    "# Label 1 for Positive and 0 for Negative reviews\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Create a feature matrix with TF-IDF\n",
    "X_tfidf = tfidf_vectorizer.transform(binary_data['review_body'])\n",
    "y = binary_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [simple_preprocess(str(doc)) for doc in binary_data[\"review_body\"]]\n",
    "self_w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_w2v_features(text, model, is_pretrained=False, vector_size=100):\n",
    "    tokens = simple_preprocess(str(text))\n",
    "    vectors = []\n",
    "    \n",
    "    if is_pretrained:\n",
    "        for word in tokens:\n",
    "            if word in model:\n",
    "                vectors.append(model[word])\n",
    "    else:\n",
    "        for word in tokens:\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "    \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:22<00:00, 8722.79it/s] \n",
      "100%|██████████| 200000/200000 [00:23<00:00, 8463.57it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_self_w2v = np.array([get_avg_w2v_features(text, self_w2v_model) for text in tqdm(binary_data['review_body'])])\n",
    "X_pretrained_w2v = np.array([get_avg_w2v_features(text, wv, is_pretrained=True, vector_size=300) for text in tqdm(binary_data['review_body'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Self-trained Word2Vec): 0.7735\n",
      "Accuracy (Pre-trained Word2Vec): 0.7435\n"
     ]
    }
   ],
   "source": [
    "X_train_self, X_test_self, y_train, y_test = train_test_split(X_self_w2v, binary_data['label'], test_size=0.2, random_state=42)\n",
    "X_train_pre, X_test_pre, _, _ = train_test_split(X_pretrained_w2v, binary_data['label'], test_size=0.2, random_state=42)\n",
    "perceptron_self = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron_self.fit(X_train_self, y_train)\n",
    "accuracy_self = accuracy_score(y_test, perceptron_self.predict(X_test_self))\n",
    "perceptron_pre = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron_pre.fit(X_train_pre, y_train)\n",
    "accuracy_pre = accuracy_score(y_test, perceptron_pre.predict(X_test_pre))\n",
    "print(f\"Accuracy (Self-trained Word2Vec): {accuracy_self:.4f}\")\n",
    "print(f\"Accuracy (Pre-trained Word2Vec): {accuracy_pre:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [00:18<00:00, 8844.03it/s]\n",
      "100%|██████████| 40000/40000 [00:04<00:00, 8841.56it/s] \n",
      "100%|██████████| 160000/160000 [00:18<00:00, 8641.06it/s]\n",
      "100%|██████████| 40000/40000 [00:04<00:00, 8903.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local W2V SVM Accuracy: 0.818875\n",
      "Google W2V SVM Accuracy: 0.812025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "\n",
    "# Load Pre-trained Word2Vec (Google News)\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Filter only positive and negative reviews\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Prepare sentences for self-trained Word2Vec\n",
    "sentences = [simple_preprocess(str(doc)) for doc in binary_data[\"review_body\"]]\n",
    "self_w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Function to get averaged word vectors\n",
    "def get_avg_w2v_features(text, model, is_pretrained=False, vector_size=100):\n",
    "    tokens = simple_preprocess(str(text))\n",
    "    vectors = []\n",
    "    \n",
    "    if is_pretrained:\n",
    "        for word in tokens:\n",
    "            if word in model:\n",
    "                vectors.append(model[word])\n",
    "    else:\n",
    "        for word in tokens:\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "    \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "# Split dataset\n",
    "pos_reviews = binary_data[binary_data['label'] == 1]['review_body']\n",
    "neg_reviews = binary_data[binary_data['label'] == 0]['review_body']\n",
    "\n",
    "pos_x_train, pos_x_test = train_test_split(pos_reviews, test_size=0.2, random_state=3)\n",
    "neg_x_train, neg_x_test = train_test_split(neg_reviews, test_size=0.2, random_state=3)\n",
    "\n",
    "X_train_12 = np.concatenate([pos_x_train, neg_x_train])\n",
    "X_test_12 = np.concatenate([pos_x_test, neg_x_test])\n",
    "y_train_12 = np.concatenate([np.ones(len(pos_x_train)), np.zeros(len(neg_x_train))])\n",
    "y_test_12 = np.concatenate([np.ones(len(pos_x_test)), np.zeros(len(neg_x_test))])\n",
    "\n",
    "# Convert dataset to Word2Vec embeddings\n",
    "X_train_local = np.array([get_avg_w2v_features(text, self_w2v_model) for text in tqdm(X_train_12)])\n",
    "X_test_local = np.array([get_avg_w2v_features(text, self_w2v_model) for text in tqdm(X_test_12)])\n",
    "\n",
    "X_train_google = np.array([get_avg_w2v_features(text, pretrained_w2v, is_pretrained=True, vector_size=300) for text in tqdm(X_train_12)])\n",
    "X_test_google = np.array([get_avg_w2v_features(text, pretrained_w2v, is_pretrained=True, vector_size=300) for text in tqdm(X_test_12)])\n",
    "\n",
    "# Train and evaluate an SVM using self-trained Word2Vec embeddings\n",
    "svm_local = LinearSVC(random_state=3)\n",
    "svm_local.fit(X_train_local, y_train_12)\n",
    "pred_svm_local = svm_local.predict(X_test_local)\n",
    "\n",
    "print(\"Local W2V SVM Accuracy:\", accuracy_score(y_test_12, pred_svm_local))\n",
    "\n",
    "# Train and evaluate an SVM using Google News Word2Vec embeddings\n",
    "svm_google = LinearSVC(random_state=3)\n",
    "svm_google.fit(X_train_google, y_train_12)\n",
    "pred_svm_google = svm_google.predict(X_test_google)\n",
    "\n",
    "print(\"Google W2V SVM Accuracy:\", accuracy_score(y_test_12, pred_svm_google))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self trained model slightly performs better as it has more relevent knowledge of the dataset. As you can clearly see above the results on self trained model is better than the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # This should print the installed version of PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs (Test Accuracy):\n",
      "1. Binary Classification using Pretrained embeddings: 0.8010\n",
      "2. Binary Classification using Self-trained embeddings: 0.8583\n",
      "3. Ternary Classification using Pretrained embeddings: 0.6407\n",
      "4. Ternary Classification using Self-trained embeddings: 0.6927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --------------------------\n",
    "# Helper function to compute average Word2Vec vector\n",
    "# --------------------------\n",
    "def get_avg_vector(text, model):\n",
    "    tokens = simple_preprocess(str(text))\n",
    "    lookup = model.wv if hasattr(model, 'wv') else model\n",
    "    vectors = [lookup[word] for word in tokens if word in lookup.key_to_index]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "\n",
    "# Load the pretrained Google News Word2Vec model\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Prepare feature matrices for each case using average Word2Vec vectors\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "ternary_data = balanced_data.copy()\n",
    "ternary_data['label'] = ternary_data['sentiment'] - 1\n",
    "\n",
    "def generate_features(df, model):\n",
    "    return np.array([get_avg_vector(review, model) for review in df['review_body']])\n",
    "\n",
    "X_bin_pre = generate_features(binary_data, google_model)\n",
    "X_bin_self = generate_features(binary_data, w2v_model)\n",
    "y_bin = binary_data['label'].values\n",
    "\n",
    "X_tern_pre = generate_features(ternary_data, google_model)\n",
    "X_tern_self = generate_features(ternary_data, w2v_model)\n",
    "y_tern = ternary_data['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bin_pre, X_test_bin_pre, y_train_bin, y_test_bin = train_test_split(X_bin_pre, y_bin, test_size=0.2, random_state=42)\n",
    "X_train_bin_self, X_test_bin_self, _, _ = train_test_split(X_bin_self, y_bin, test_size=0.2, random_state=42)\n",
    "X_train_tern_pre, X_test_tern_pre, y_train_tern, y_test_tern = train_test_split(X_tern_pre, y_tern, test_size=0.2, random_state=42)\n",
    "X_train_tern_self, X_test_tern_self, _, _ = train_test_split(X_tern_self, y_tern, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "def to_tensor(X, y=None):\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    y_tensor = torch.LongTensor(y).to(device) if y is not None else None\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train_bin_pre, y_train_bin = to_tensor(X_train_bin_pre, y_train_bin)\n",
    "X_test_bin_pre, y_test_bin = to_tensor(X_test_bin_pre, y_test_bin)\n",
    "X_train_bin_self, _ = to_tensor(X_train_bin_self)\n",
    "X_test_bin_self, _ = to_tensor(X_test_bin_self)\n",
    "X_train_tern_pre, y_train_tern = to_tensor(X_train_tern_pre, y_train_tern)\n",
    "X_test_tern_pre, y_test_tern = to_tensor(X_test_tern_pre, y_test_tern)\n",
    "X_train_tern_self, _ = to_tensor(X_train_tern_self)\n",
    "X_test_tern_self, _ = to_tensor(X_test_tern_self)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1, hidden2, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model and return its test accuracy\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.001):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return test_acc\n",
    "\n",
    "# Train and evaluate models\n",
    "model_bin_pre = MLP(300, 50, 10, 2)\n",
    "acc_bin_pre = train_model(model_bin_pre, X_train_bin_pre, y_train_bin, X_test_bin_pre, y_test_bin)\n",
    "\n",
    "model_bin_self = MLP(300, 50, 10, 2)\n",
    "acc_bin_self = train_model(model_bin_self, X_train_bin_self, y_train_bin, X_test_bin_self, y_test_bin)\n",
    "\n",
    "model_tern_pre = MLP(300, 50, 10, 3)\n",
    "acc_tern_pre = train_model(model_tern_pre, X_train_tern_pre, y_train_tern, X_test_tern_pre, y_test_tern)\n",
    "\n",
    "model_tern_self = MLP(300, 50, 10, 3)\n",
    "acc_tern_self = train_model(model_tern_self, X_train_tern_self, y_train_tern, X_test_tern_self, y_test_tern)\n",
    "\n",
    "# Report the test accuracy for each model\n",
    "print(\"Model outputs (Test Accuracy):\")\n",
    "print(\"1. Binary Classification using Pretrained embeddings: {:.4f}\".format(acc_bin_pre))\n",
    "print(\"2. Binary Classification using Self-trained embeddings: {:.4f}\".format(acc_bin_self))\n",
    "print(\"3. Ternary Classification using Pretrained embeddings: {:.4f}\".format(acc_tern_pre))\n",
    "print(\"4. Ternary Classification using Self-trained embeddings: {:.4f}\".format(acc_tern_self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary model performs better than that of the ternary as we can see a significant jump in the accuracies.\n",
    "# Additionally, the self trained model performs better overall than the pretained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating concat features: 100%|██████████| 200000/200000 [00:12<00:00, 16616.42it/s]\n",
      "Training epochs:   2%|▏         | 1/50 [00:05<04:08,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 -- Loss: 0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   4%|▍         | 2/50 [00:09<03:54,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 -- Loss: 0.4239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   6%|▌         | 3/50 [00:14<03:53,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 -- Loss: 0.3852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   8%|▊         | 4/50 [00:19<03:46,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 -- Loss: 0.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 5/50 [00:24<03:38,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 -- Loss: 0.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  12%|█▏        | 6/50 [00:29<03:32,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 -- Loss: 0.2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  14%|█▍        | 7/50 [00:33<03:26,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 -- Loss: 0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  16%|█▌        | 8/50 [00:38<03:22,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 -- Loss: 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  18%|█▊        | 9/50 [00:43<03:16,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 -- Loss: 0.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 10/50 [00:48<03:10,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 -- Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  22%|██▏       | 11/50 [00:53<03:09,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 -- Loss: 0.1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  24%|██▍       | 12/50 [00:58<03:05,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 -- Loss: 0.1528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  26%|██▌       | 13/50 [01:02<02:58,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 -- Loss: 0.1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  28%|██▊       | 14/50 [01:08<02:59,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 -- Loss: 0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  30%|███       | 15/50 [01:13<02:59,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 -- Loss: 0.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  32%|███▏      | 16/50 [01:19<03:01,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 -- Loss: 0.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  34%|███▍      | 17/50 [01:25<02:59,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 -- Loss: 0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  36%|███▌      | 18/50 [01:30<02:54,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 -- Loss: 0.1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  38%|███▊      | 19/50 [01:36<02:49,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 -- Loss: 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  40%|████      | 20/50 [01:42<02:47,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 -- Loss: 0.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  42%|████▏     | 21/50 [01:47<02:42,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 -- Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  44%|████▍     | 22/50 [01:53<02:36,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 -- Loss: 0.0814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  46%|████▌     | 23/50 [01:58<02:29,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 -- Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  48%|████▊     | 24/50 [02:04<02:24,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 -- Loss: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  50%|█████     | 25/50 [02:10<02:21,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 -- Loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  52%|█████▏    | 26/50 [02:15<02:15,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 -- Loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  54%|█████▍    | 27/50 [02:21<02:08,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 -- Loss: 0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  56%|█████▌    | 28/50 [02:27<02:02,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 -- Loss: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  58%|█████▊    | 29/50 [02:32<01:58,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 -- Loss: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  60%|██████    | 30/50 [02:38<01:53,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 -- Loss: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  62%|██████▏   | 31/50 [02:44<01:46,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 -- Loss: 0.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  64%|██████▍   | 32/50 [02:49<01:40,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 -- Loss: 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  66%|██████▌   | 33/50 [02:55<01:35,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 -- Loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  68%|██████▊   | 34/50 [03:01<01:30,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 -- Loss: 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  70%|███████   | 35/50 [03:06<01:24,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 -- Loss: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  72%|███████▏  | 36/50 [03:12<01:18,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 -- Loss: 0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  74%|███████▍  | 37/50 [03:17<01:12,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 -- Loss: 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  76%|███████▌  | 38/50 [03:23<01:07,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 -- Loss: 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  78%|███████▊  | 39/50 [03:29<01:02,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 -- Loss: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  80%|████████  | 40/50 [03:34<00:56,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 -- Loss: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  82%|████████▏ | 41/50 [03:40<00:50,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 -- Loss: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  84%|████████▍ | 42/50 [03:45<00:45,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 -- Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  86%|████████▌ | 43/50 [03:51<00:39,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 -- Loss: 0.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  88%|████████▊ | 44/50 [03:57<00:33,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 -- Loss: 0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  90%|█████████ | 45/50 [04:02<00:27,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 -- Loss: 0.0395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  92%|█████████▏| 46/50 [04:08<00:22,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 -- Loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  94%|█████████▍| 47/50 [04:14<00:16,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 -- Loss: 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  96%|█████████▌| 48/50 [04:19<00:11,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 -- Loss: 0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  98%|█████████▊| 49/50 [04:25<00:05,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 -- Loss: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 50/50 [04:30<00:00,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 -- Loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification using Pretrained embeddings Accuracy: 0.7523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# 1. Feature Extraction Function:\n",
    "#    Concatenate the first 10 Word2Vec vectors for each review.\n",
    "# --------------------------\n",
    "def get_concat_vector(text, model, max_tokens=10):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using gensim's simple_preprocess and concatenates\n",
    "    the first max_tokens word vectors from the provided Word2Vec model.\n",
    "    If there are fewer than max_tokens tokens, pads with zero vectors.\n",
    "    Returns a feature vector of dimension (max_tokens * embedding_dim), i.e. (10 * 300 = 3000).\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(str(text))\n",
    "    lookup = model.wv if hasattr(model, 'wv') else model\n",
    "    vectors = []\n",
    "    for token in tokens[:max_tokens]:\n",
    "        if token in lookup.key_to_index:\n",
    "            vectors.append(lookup[token])\n",
    "        else:\n",
    "            vectors.append(np.zeros(lookup.vector_size))\n",
    "    while len(vectors) < max_tokens:\n",
    "        vectors.append(np.zeros(lookup.vector_size))\n",
    "    vec = np.concatenate(vectors)\n",
    "    # Check the resulting dimension is as expected.\n",
    "    assert vec.shape[0] == max_tokens * lookup.vector_size, (\n",
    "        f\"Expected dimension {max_tokens * lookup.vector_size}, got {vec.shape[0]}\"\n",
    "    )\n",
    "    return vec\n",
    "\n",
    "def generate_concat_features(df, model):\n",
    "    \"\"\"\n",
    "    Generates a feature matrix (n_samples x 3000) where each row is the concatenation\n",
    "    of the first 10 Word2Vec vectors from df['review_body'].\n",
    "    Uses tqdm to display progress.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for review in tqdm(df['review_body'], desc=\"Generating concat features\"):\n",
    "        features.append(get_concat_vector(review, model))\n",
    "    return np.array(features)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Prepare the Binary Dataset\n",
    "# --------------------------\n",
    "# Filter reviews to only those with sentiment 1 or 2.\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "# Map sentiment 1 -> label 0 and sentiment 2 -> label 1.\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Load the Pretrained Google News Word2Vec Model\n",
    "# --------------------------\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# --------------------------\n",
    "# 4. Generate Input Features using Pretrained Embeddings\n",
    "# --------------------------\n",
    "X_bin_pre = generate_concat_features(binary_data, google_model)\n",
    "y_bin = binary_data['label'].values\n",
    "\n",
    "# --------------------------\n",
    "# 5. Split the Data (80/20 Train/Test)\n",
    "# --------------------------\n",
    "X_train_pre, X_test_pre, y_train, y_test = train_test_split(\n",
    "    X_bin_pre, y_bin, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6. Convert Data to PyTorch Tensors\n",
    "# --------------------------\n",
    "def to_tensor(X, y=None):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    if y is not None:\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        return X_tensor, y_tensor\n",
    "    return X_tensor\n",
    "\n",
    "X_train_pre, y_train = to_tensor(X_train_pre, y_train)\n",
    "X_test_pre, y_test = to_tensor(X_test_pre, y_test)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Define the MLP Model Architecture\n",
    "# --------------------------\n",
    "# Input dimension is 3000 (10 tokens * 300 dims), two hidden layers (50 and 10 nodes) and output (2 classes).\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=50, hidden2=10, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------\n",
    "# 8. Define the Training Function with Mini-Batching and tqdm\n",
    "# --------------------------\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.001, batch_size=32):\n",
    "    # Create a DataLoader for mini-batch training.\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate the model on the test set.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_acc = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "    return test_acc\n",
    "\n",
    "# --------------------------\n",
    "# 9. Build, Train, and Evaluate the Binary Classification Model using Pretrained embeddings.\n",
    "# --------------------------\n",
    "input_dim = 3000  # 10 tokens * 300 dimensions\n",
    "model_bin_pre = MLP(input_dim=input_dim, hidden1=50, hidden2=10, output_dim=2)\n",
    "acc_bin_pre = train_model(model_bin_pre, X_train_pre, y_train, X_test_pre, y_test,\n",
    "                          epochs=50, lr=0.001, batch_size=32)\n",
    "\n",
    "print(\"Binary Classification using Pretrained embeddings Accuracy: {:.4f}\".format(acc_bin_pre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   1%|          | 1/100 [00:03<05:47,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 -- Loss: 0.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   2%|▏         | 2/100 [00:07<05:45,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 -- Loss: 0.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   3%|▎         | 3/100 [00:10<05:42,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 -- Loss: 0.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   4%|▍         | 4/100 [00:14<05:40,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 -- Loss: 0.2745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [00:17<05:37,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 -- Loss: 0.2681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   6%|▌         | 6/100 [00:21<05:33,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 -- Loss: 0.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   7%|▋         | 7/100 [00:24<05:31,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 -- Loss: 0.2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   8%|▊         | 8/100 [00:28<05:27,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 -- Loss: 0.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   9%|▉         | 9/100 [00:31<05:23,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 -- Loss: 0.2510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [00:35<05:19,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 -- Loss: 0.2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  11%|█         | 11/100 [00:38<05:14,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 -- Loss: 0.2453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  12%|█▏        | 12/100 [00:42<05:12,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 -- Loss: 0.2422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  13%|█▎        | 13/100 [00:46<05:09,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 -- Loss: 0.2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  14%|█▍        | 14/100 [00:49<05:04,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 -- Loss: 0.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [00:53<05:01,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 -- Loss: 0.2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  16%|█▌        | 16/100 [00:57<05:05,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 -- Loss: 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  17%|█▋        | 17/100 [01:00<05:02,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 -- Loss: 0.2318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  18%|█▊        | 18/100 [01:04<04:59,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 -- Loss: 0.2307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  19%|█▉        | 19/100 [01:08<04:55,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 -- Loss: 0.2288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 20/100 [01:11<04:51,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 -- Loss: 0.2272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  21%|██        | 21/100 [01:15<04:47,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 -- Loss: 0.2259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  22%|██▏       | 22/100 [01:18<04:44,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 -- Loss: 0.2242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  23%|██▎       | 23/100 [01:22<04:39,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 -- Loss: 0.2235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  24%|██▍       | 24/100 [01:26<04:37,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 -- Loss: 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  25%|██▌       | 25/100 [01:29<04:30,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 -- Loss: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  26%|██▌       | 26/100 [01:33<04:25,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 -- Loss: 0.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  27%|██▋       | 27/100 [01:36<04:22,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 -- Loss: 0.2185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  28%|██▊       | 28/100 [01:40<04:17,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 -- Loss: 0.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  29%|██▉       | 29/100 [01:43<04:12,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 -- Loss: 0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  30%|███       | 30/100 [01:47<04:09,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 -- Loss: 0.2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  31%|███       | 31/100 [01:51<04:04,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 -- Loss: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  32%|███▏      | 32/100 [01:54<04:01,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 -- Loss: 0.2139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  33%|███▎      | 33/100 [01:58<03:58,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 -- Loss: 0.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  34%|███▍      | 34/100 [02:01<03:53,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 -- Loss: 0.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  35%|███▌      | 35/100 [02:05<03:50,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 -- Loss: 0.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  36%|███▌      | 36/100 [02:08<03:46,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 -- Loss: 0.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  37%|███▋      | 37/100 [02:12<03:41,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 -- Loss: 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  38%|███▊      | 38/100 [02:15<03:39,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 -- Loss: 0.2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  39%|███▉      | 39/100 [02:19<03:36,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 -- Loss: 0.2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  40%|████      | 40/100 [02:22<03:32,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 -- Loss: 0.2069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  41%|████      | 41/100 [02:26<03:28,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 -- Loss: 0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  42%|████▏     | 42/100 [02:29<03:25,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 -- Loss: 0.2053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  43%|████▎     | 43/100 [02:33<03:20,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 -- Loss: 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  44%|████▍     | 44/100 [02:37<03:18,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 -- Loss: 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  45%|████▌     | 45/100 [02:40<03:15,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 -- Loss: 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  46%|████▌     | 46/100 [02:44<03:10,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 -- Loss: 0.2028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  47%|████▋     | 47/100 [02:47<03:07,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 -- Loss: 0.2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  48%|████▊     | 48/100 [02:51<03:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 -- Loss: 0.2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  49%|████▉     | 49/100 [02:54<02:59,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 -- Loss: 0.2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  50%|█████     | 50/100 [02:58<02:57,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 -- Loss: 0.2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  51%|█████     | 51/100 [03:01<02:54,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 -- Loss: 0.2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  52%|█████▏    | 52/100 [03:05<02:49,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 -- Loss: 0.1996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  53%|█████▎    | 53/100 [03:08<02:46,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 -- Loss: 0.1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  54%|█████▍    | 54/100 [03:12<02:42,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 -- Loss: 0.1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  55%|█████▌    | 55/100 [03:15<02:39,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 -- Loss: 0.1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  56%|█████▌    | 56/100 [03:19<02:35,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 -- Loss: 0.1973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  57%|█████▋    | 57/100 [03:23<02:32,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 -- Loss: 0.1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  58%|█████▊    | 58/100 [03:26<02:27,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 -- Loss: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  59%|█████▉    | 59/100 [03:30<02:24,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 -- Loss: 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  60%|██████    | 60/100 [03:33<02:21,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 -- Loss: 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  61%|██████    | 61/100 [03:37<02:18,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 -- Loss: 0.1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  62%|██████▏   | 62/100 [03:40<02:14,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 -- Loss: 0.1948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  63%|██████▎   | 63/100 [03:44<02:11,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 -- Loss: 0.1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  64%|██████▍   | 64/100 [03:47<02:07,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 -- Loss: 0.1935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  65%|██████▌   | 65/100 [03:51<02:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 -- Loss: 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  66%|██████▌   | 66/100 [03:54<02:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 -- Loss: 0.1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  67%|██████▋   | 67/100 [03:58<01:56,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 -- Loss: 0.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  68%|██████▊   | 68/100 [04:01<01:53,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 -- Loss: 0.1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  69%|██████▉   | 69/100 [04:05<01:49,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 -- Loss: 0.1916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  70%|███████   | 70/100 [04:09<01:46,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 -- Loss: 0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  71%|███████   | 71/100 [04:12<01:42,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 -- Loss: 0.1906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  72%|███████▏  | 72/100 [04:16<01:39,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 -- Loss: 0.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  73%|███████▎  | 73/100 [04:19<01:36,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 -- Loss: 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  74%|███████▍  | 74/100 [04:23<01:32,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 -- Loss: 0.1895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  75%|███████▌  | 75/100 [04:26<01:28,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 -- Loss: 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  76%|███████▌  | 76/100 [04:30<01:25,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 -- Loss: 0.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  77%|███████▋  | 77/100 [04:33<01:21,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 -- Loss: 0.1886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  78%|███████▊  | 78/100 [04:37<01:18,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 -- Loss: 0.1885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  79%|███████▉  | 79/100 [04:41<01:14,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 -- Loss: 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  80%|████████  | 80/100 [04:44<01:10,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 -- Loss: 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  81%|████████  | 81/100 [04:48<01:07,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 -- Loss: 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  82%|████████▏ | 82/100 [04:51<01:03,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 -- Loss: 0.1868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  83%|████████▎ | 83/100 [04:55<01:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 -- Loss: 0.1871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  84%|████████▍ | 84/100 [04:58<00:56,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 -- Loss: 0.1860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  85%|████████▌ | 85/100 [05:02<00:53,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 -- Loss: 0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  86%|████████▌ | 86/100 [05:05<00:49,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 -- Loss: 0.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  87%|████████▋ | 87/100 [05:09<00:46,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 -- Loss: 0.1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  88%|████████▊ | 88/100 [05:12<00:42,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 -- Loss: 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  89%|████████▉ | 89/100 [05:16<00:39,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 -- Loss: 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  90%|█████████ | 90/100 [05:20<00:35,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 -- Loss: 0.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  91%|█████████ | 91/100 [05:23<00:31,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 -- Loss: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  92%|█████████▏| 92/100 [05:27<00:28,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 -- Loss: 0.1840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  93%|█████████▎| 93/100 [05:30<00:24,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 -- Loss: 0.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  94%|█████████▍| 94/100 [05:34<00:21,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 -- Loss: 0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  95%|█████████▌| 95/100 [05:37<00:17,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 -- Loss: 0.1834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  96%|█████████▌| 96/100 [05:41<00:14,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 -- Loss: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  97%|█████████▋| 97/100 [05:44<00:10,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 -- Loss: 0.1823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  98%|█████████▊| 98/100 [05:48<00:07,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 -- Loss: 0.1823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  99%|█████████▉| 99/100 [05:52<00:03,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 -- Loss: 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 100/100 [05:55<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 -- Loss: 0.1814\n",
      "Binary Classification using Self-trained embeddings Accuracy: 0.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For binary classification, only consider sentiment 1 and 2.\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "# Map sentiment 1 to label 0 and sentiment 2 to label 1.\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# For ternary classification, use all classes 1, 2, and 3; remap to 0, 1, 2.\n",
    "ternary_data = balanced_data.copy()\n",
    "ternary_data['label'] = ternary_data['sentiment'] - 1\n",
    "\n",
    "# Using average Word2Vec vectors (300-dim)\n",
    "# Pretrained features:\n",
    "X_bin_pre = generate_features(binary_data, google_model)  \n",
    "# Self-trained features:\n",
    "X_bin_self = generate_features(binary_data, w2v_model)  \n",
    "y_bin = binary_data['label'].values\n",
    "\n",
    "# For ternary data:\n",
    "X_tern_pre = generate_features(ternary_data, google_model)\n",
    "X_tern_self = generate_features(ternary_data, w2v_model)\n",
    "y_tern = ternary_data['label'].values\n",
    "\n",
    "# Binary splits:\n",
    "X_train_bin_pre, X_test_bin_pre, y_train_bin, y_test_bin = train_test_split(X_bin_pre, y_bin, test_size=0.2, random_state=42)\n",
    "X_train_bin_self, X_test_bin_self, _, _ = train_test_split(X_bin_self, y_bin, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ternary splits:\n",
    "X_train_tern_pre, X_test_tern_pre, y_train_tern, y_test_tern = train_test_split(X_tern_pre, y_tern, test_size=0.2, random_state=42)\n",
    "X_train_tern_self, X_test_tern_self, _, _ = train_test_split(X_tern_self, y_tern, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors:\n",
    "X_train_bin_pre, y_train_bin = to_tensor(X_train_bin_pre, y_train_bin)\n",
    "X_test_bin_pre, y_test_bin   = to_tensor(X_test_bin_pre, y_test_bin)\n",
    "X_train_bin_self = to_tensor(X_train_bin_self, None)\n",
    "X_test_bin_self  = to_tensor(X_test_bin_self, None)\n",
    "\n",
    "\n",
    "X_train_tern_pre, y_train_tern = to_tensor(X_train_tern_pre, y_train_tern)\n",
    "X_test_tern_pre, y_test_tern   = to_tensor(X_test_tern_pre, y_test_tern)\n",
    "X_train_tern_self = to_tensor(X_train_tern_self, None)\n",
    "X_test_tern_self  = to_tensor(X_test_tern_self, None)\n",
    "\n",
    "\n",
    "# Build the MLP for binary classification (self-trained embeddings)\n",
    "# Input dimension remains 300 (since we're averaging the word vectors)\n",
    "model_bin_self = MLP(input_dim=300, hidden1=50, hidden2=10, output_dim=2)\n",
    "acc_bin_self = train_model(model_bin_self, X_train_bin_self, y_train_bin, X_test_bin_self, y_test_bin, epochs=100, lr=0.001)\n",
    "print(\"Binary Classification using Self-trained embeddings Accuracy: {:.4f}\".format(acc_bin_self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   1%|          | 1/100 [00:04<07:27,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 -- Loss: 0.7735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   2%|▏         | 2/100 [00:08<07:16,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 -- Loss: 0.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   3%|▎         | 3/100 [00:13<07:13,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 -- Loss: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   4%|▍         | 4/100 [00:17<07:09,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 -- Loss: 0.7098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [00:22<07:02,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 -- Loss: 0.7035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   6%|▌         | 6/100 [00:26<06:58,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 -- Loss: 0.6981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   7%|▋         | 7/100 [00:31<06:53,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 -- Loss: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   8%|▊         | 8/100 [00:35<06:49,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 -- Loss: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   9%|▉         | 9/100 [00:40<06:44,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 -- Loss: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [00:44<06:38,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 -- Loss: 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  11%|█         | 11/100 [00:48<06:35,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 -- Loss: 0.6791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  12%|█▏        | 12/100 [00:53<06:33,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 -- Loss: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  13%|█▎        | 13/100 [00:58<06:30,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 -- Loss: 0.6733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  14%|█▍        | 14/100 [01:02<06:26,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 -- Loss: 0.6710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [01:07<06:23,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 -- Loss: 0.6691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  16%|█▌        | 16/100 [01:11<06:19,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 -- Loss: 0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  17%|█▋        | 17/100 [01:16<06:16,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 -- Loss: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  18%|█▊        | 18/100 [01:20<06:13,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 -- Loss: 0.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  19%|█▉        | 19/100 [01:25<06:08,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 -- Loss: 0.6618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 20/100 [01:29<06:02,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 -- Loss: 0.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  21%|██        | 21/100 [01:34<06:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 -- Loss: 0.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  22%|██▏       | 22/100 [01:38<05:55,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 -- Loss: 0.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  23%|██▎       | 23/100 [01:43<05:50,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 -- Loss: 0.6557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  24%|██▍       | 24/100 [01:48<05:45,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 -- Loss: 0.6550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  25%|██▌       | 25/100 [01:52<05:39,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 -- Loss: 0.6534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  26%|██▌       | 26/100 [01:57<05:36,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 -- Loss: 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  27%|██▋       | 27/100 [02:01<05:31,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 -- Loss: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  28%|██▊       | 28/100 [02:06<05:27,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 -- Loss: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  29%|██▉       | 29/100 [02:10<05:22,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 -- Loss: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  30%|███       | 30/100 [02:15<05:16,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 -- Loss: 0.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  31%|███       | 31/100 [02:19<05:12,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 -- Loss: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  32%|███▏      | 32/100 [02:24<05:07,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 -- Loss: 0.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  33%|███▎      | 33/100 [02:28<05:03,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 -- Loss: 0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  34%|███▍      | 34/100 [02:33<04:57,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 -- Loss: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  35%|███▌      | 35/100 [02:37<04:55,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 -- Loss: 0.6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  36%|███▌      | 36/100 [02:42<04:50,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 -- Loss: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  37%|███▋      | 37/100 [02:46<04:45,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 -- Loss: 0.6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  38%|███▊      | 38/100 [02:51<04:40,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 -- Loss: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  39%|███▉      | 39/100 [02:56<04:36,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 -- Loss: 0.6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  40%|████      | 40/100 [03:00<04:32,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 -- Loss: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  41%|████      | 41/100 [03:05<04:27,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 -- Loss: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  42%|████▏     | 42/100 [03:09<04:23,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 -- Loss: 0.6382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  43%|████▎     | 43/100 [03:14<04:18,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 -- Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  44%|████▍     | 44/100 [03:18<04:13,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 -- Loss: 0.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  45%|████▌     | 45/100 [03:23<04:08,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 -- Loss: 0.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  46%|████▌     | 46/100 [03:27<04:04,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 -- Loss: 0.6354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  47%|████▋     | 47/100 [03:32<03:59,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 -- Loss: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  48%|████▊     | 48/100 [03:36<03:55,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 -- Loss: 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  49%|████▉     | 49/100 [03:41<03:51,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 -- Loss: 0.6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  50%|█████     | 50/100 [03:45<03:46,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 -- Loss: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  51%|█████     | 51/100 [03:50<03:41,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 -- Loss: 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  52%|█████▏    | 52/100 [03:54<03:36,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 -- Loss: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  53%|█████▎    | 53/100 [03:59<03:32,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 -- Loss: 0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  54%|█████▍    | 54/100 [04:03<03:27,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 -- Loss: 0.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  55%|█████▌    | 55/100 [04:08<03:23,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 -- Loss: 0.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  56%|█████▌    | 56/100 [04:13<03:20,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 -- Loss: 0.6302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  57%|█████▋    | 57/100 [04:17<03:14,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 -- Loss: 0.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  58%|█████▊    | 58/100 [04:22<03:10,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 -- Loss: 0.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  59%|█████▉    | 59/100 [04:26<03:05,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 -- Loss: 0.6287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  60%|██████    | 60/100 [04:31<03:01,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 -- Loss: 0.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  61%|██████    | 61/100 [04:35<02:56,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 -- Loss: 0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  62%|██████▏   | 62/100 [04:40<02:51,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 -- Loss: 0.6274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  63%|██████▎   | 63/100 [04:44<02:47,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 -- Loss: 0.6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  64%|██████▍   | 64/100 [04:49<02:43,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 -- Loss: 0.6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  65%|██████▌   | 65/100 [04:53<02:38,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 -- Loss: 0.6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  66%|██████▌   | 66/100 [04:58<02:33,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 -- Loss: 0.6262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  67%|██████▋   | 67/100 [05:02<02:28,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 -- Loss: 0.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  68%|██████▊   | 68/100 [05:07<02:24,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 -- Loss: 0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  69%|██████▉   | 69/100 [05:11<02:19,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 -- Loss: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  70%|███████   | 70/100 [05:16<02:15,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 -- Loss: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  71%|███████   | 71/100 [05:20<02:11,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 -- Loss: 0.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  72%|███████▏  | 72/100 [05:25<02:06,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 -- Loss: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  73%|███████▎  | 73/100 [05:29<02:01,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 -- Loss: 0.6234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  74%|███████▍  | 74/100 [05:34<01:57,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 -- Loss: 0.6231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  75%|███████▌  | 75/100 [05:38<01:52,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 -- Loss: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  76%|███████▌  | 76/100 [05:43<01:48,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 -- Loss: 0.6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  77%|███████▋  | 77/100 [05:47<01:43,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 -- Loss: 0.6220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  78%|███████▊  | 78/100 [05:52<01:39,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 -- Loss: 0.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  79%|███████▉  | 79/100 [05:56<01:34,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 -- Loss: 0.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  80%|████████  | 80/100 [06:01<01:30,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 -- Loss: 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  81%|████████  | 81/100 [06:05<01:25,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 -- Loss: 0.6209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  82%|████████▏ | 82/100 [06:10<01:21,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 -- Loss: 0.6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  83%|████████▎ | 83/100 [06:14<01:16,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 -- Loss: 0.6202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  84%|████████▍ | 84/100 [06:19<01:12,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 -- Loss: 0.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  85%|████████▌ | 85/100 [06:24<01:07,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 -- Loss: 0.6193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  86%|████████▌ | 86/100 [06:28<01:03,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 -- Loss: 0.6196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  87%|████████▋ | 87/100 [06:33<00:58,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 -- Loss: 0.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  88%|████████▊ | 88/100 [06:37<00:54,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 -- Loss: 0.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  89%|████████▉ | 89/100 [06:42<00:49,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 -- Loss: 0.6182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  90%|█████████ | 90/100 [06:46<00:45,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 -- Loss: 0.6181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  91%|█████████ | 91/100 [06:51<00:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 -- Loss: 0.6180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  92%|█████████▏| 92/100 [06:55<00:36,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 -- Loss: 0.6178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  93%|█████████▎| 93/100 [07:00<00:31,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 -- Loss: 0.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  94%|█████████▍| 94/100 [07:04<00:27,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 -- Loss: 0.6168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  95%|█████████▌| 95/100 [07:09<00:22,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 -- Loss: 0.6168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  96%|█████████▌| 96/100 [07:13<00:18,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 -- Loss: 0.6161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  97%|█████████▋| 97/100 [07:18<00:13,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 -- Loss: 0.6163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  98%|█████████▊| 98/100 [07:22<00:09,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 -- Loss: 0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  99%|█████████▉| 99/100 [07:27<00:04,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 -- Loss: 0.6156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 100/100 [07:31<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 -- Loss: 0.6156\n",
      "Ternary Classification using Pretrained embeddings Accuracy: 0.7003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the MLP for ternary classification (pretrained embeddings)\n",
    "model_tern_pre = MLP(input_dim=300, hidden1=50, hidden2=10, output_dim=3)\n",
    "acc_tern_pre = train_model(model_tern_pre, X_train_tern_pre, y_train_tern, X_test_tern_pre, y_test_tern, epochs=100, lr=0.001)\n",
    "print(\"Ternary Classification using Pretrained embeddings Accuracy: {:.4f}\".format(acc_tern_pre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   1%|          | 1/100 [00:04<07:22,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 -- Loss: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   2%|▏         | 2/100 [00:08<07:16,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 -- Loss: 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   3%|▎         | 3/100 [00:13<07:11,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 -- Loss: 0.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   4%|▍         | 4/100 [00:17<07:06,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 -- Loss: 0.6497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [00:22<07:02,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 -- Loss: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   6%|▌         | 6/100 [00:26<06:57,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 -- Loss: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   7%|▋         | 7/100 [00:31<06:53,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 -- Loss: 0.6327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   8%|▊         | 8/100 [00:35<06:48,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 -- Loss: 0.6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   9%|▉         | 9/100 [00:40<06:45,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 -- Loss: 0.6254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [00:44<06:41,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 -- Loss: 0.6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  11%|█         | 11/100 [00:48<06:36,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 -- Loss: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  12%|█▏        | 12/100 [00:53<06:31,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 -- Loss: 0.6176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  13%|█▎        | 13/100 [00:57<06:28,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 -- Loss: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  14%|█▍        | 14/100 [01:02<06:23,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 -- Loss: 0.6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [01:06<06:18,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 -- Loss: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  16%|█▌        | 16/100 [01:11<06:13,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 -- Loss: 0.6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  17%|█▋        | 17/100 [01:15<06:08,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 -- Loss: 0.6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  18%|█▊        | 18/100 [01:20<06:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 -- Loss: 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  19%|█▉        | 19/100 [01:24<06:00,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 -- Loss: 0.6052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 20/100 [01:28<05:55,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 -- Loss: 0.6038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  21%|██        | 21/100 [01:33<05:51,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 -- Loss: 0.6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  22%|██▏       | 22/100 [01:37<05:47,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 -- Loss: 0.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  23%|██▎       | 23/100 [01:42<05:43,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 -- Loss: 0.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  24%|██▍       | 24/100 [01:46<05:38,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 -- Loss: 0.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  25%|██▌       | 25/100 [01:51<05:33,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 -- Loss: 0.5974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  26%|██▌       | 26/100 [01:55<05:29,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 -- Loss: 0.5968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  27%|██▋       | 27/100 [02:00<05:24,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 -- Loss: 0.5961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  28%|██▊       | 28/100 [02:04<05:19,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 -- Loss: 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  29%|██▉       | 29/100 [02:09<05:16,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 -- Loss: 0.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  30%|███       | 30/100 [02:13<05:11,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 -- Loss: 0.5935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  31%|███       | 31/100 [02:17<05:06,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 -- Loss: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  32%|███▏      | 32/100 [02:22<05:02,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 -- Loss: 0.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  33%|███▎      | 33/100 [02:26<04:57,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 -- Loss: 0.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  34%|███▍      | 34/100 [02:31<04:54,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 -- Loss: 0.5902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  35%|███▌      | 35/100 [02:35<04:49,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 -- Loss: 0.5898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  36%|███▌      | 36/100 [02:40<04:44,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 -- Loss: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  37%|███▋      | 37/100 [02:44<04:41,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 -- Loss: 0.5883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  38%|███▊      | 38/100 [02:49<04:36,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 -- Loss: 0.5879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  39%|███▉      | 39/100 [02:53<04:31,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 -- Loss: 0.5870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  40%|████      | 40/100 [02:57<04:26,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 -- Loss: 0.5870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  41%|████      | 41/100 [03:02<04:21,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 -- Loss: 0.5864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  42%|████▏     | 42/100 [03:06<04:16,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 -- Loss: 0.5853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  43%|████▎     | 43/100 [03:11<04:12,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 -- Loss: 0.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  44%|████▍     | 44/100 [03:15<04:07,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 -- Loss: 0.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  45%|████▌     | 45/100 [03:20<04:03,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 -- Loss: 0.5843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  46%|████▌     | 46/100 [03:24<03:59,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 -- Loss: 0.5835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  47%|████▋     | 47/100 [03:28<03:54,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 -- Loss: 0.5831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  48%|████▊     | 48/100 [03:33<03:50,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 -- Loss: 0.5828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  49%|████▉     | 49/100 [03:37<03:46,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 -- Loss: 0.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  50%|█████     | 50/100 [03:42<03:42,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 -- Loss: 0.5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  51%|█████     | 51/100 [03:46<03:37,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 -- Loss: 0.5812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  52%|█████▏    | 52/100 [03:51<03:32,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 -- Loss: 0.5812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  53%|█████▎    | 53/100 [03:55<03:27,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 -- Loss: 0.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  54%|█████▍    | 54/100 [03:59<03:23,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 -- Loss: 0.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  55%|█████▌    | 55/100 [04:04<03:19,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 -- Loss: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  56%|█████▌    | 56/100 [04:08<03:15,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 -- Loss: 0.5791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  57%|█████▋    | 57/100 [04:13<03:10,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 -- Loss: 0.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  58%|█████▊    | 58/100 [04:17<03:06,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 -- Loss: 0.5783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  59%|█████▉    | 59/100 [04:22<03:02,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 -- Loss: 0.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  60%|██████    | 60/100 [04:26<02:57,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 -- Loss: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  61%|██████    | 61/100 [04:31<02:53,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 -- Loss: 0.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  62%|██████▏   | 62/100 [04:35<02:48,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 -- Loss: 0.5774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  63%|██████▎   | 63/100 [04:39<02:43,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 -- Loss: 0.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  64%|██████▍   | 64/100 [04:44<02:39,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 -- Loss: 0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  65%|██████▌   | 65/100 [04:48<02:34,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 -- Loss: 0.5767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  66%|██████▌   | 66/100 [04:53<02:30,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 -- Loss: 0.5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  67%|██████▋   | 67/100 [04:57<02:26,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 -- Loss: 0.5756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  68%|██████▊   | 68/100 [05:02<02:21,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 -- Loss: 0.5755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  69%|██████▉   | 69/100 [05:06<02:18,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 -- Loss: 0.5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  70%|███████   | 70/100 [05:11<02:13,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 -- Loss: 0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  71%|███████   | 71/100 [05:15<02:09,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 -- Loss: 0.5748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  72%|███████▏  | 72/100 [05:19<02:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 -- Loss: 0.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  73%|███████▎  | 73/100 [05:24<01:59,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 -- Loss: 0.5744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  74%|███████▍  | 74/100 [05:28<01:55,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 -- Loss: 0.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  75%|███████▌  | 75/100 [05:33<01:50,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 -- Loss: 0.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  76%|███████▌  | 76/100 [05:37<01:46,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 -- Loss: 0.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  77%|███████▋  | 77/100 [05:42<01:41,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 -- Loss: 0.5729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  78%|███████▊  | 78/100 [05:46<01:37,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 -- Loss: 0.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  79%|███████▉  | 79/100 [05:50<01:33,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 -- Loss: 0.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  80%|████████  | 80/100 [05:55<01:28,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 -- Loss: 0.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  81%|████████  | 81/100 [05:59<01:24,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 -- Loss: 0.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  82%|████████▏ | 82/100 [06:04<01:19,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 -- Loss: 0.5716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  83%|████████▎ | 83/100 [06:08<01:15,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 -- Loss: 0.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  84%|████████▍ | 84/100 [06:13<01:10,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 -- Loss: 0.5717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  85%|████████▌ | 85/100 [06:17<01:06,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 -- Loss: 0.5711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  86%|████████▌ | 86/100 [06:21<01:01,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 -- Loss: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  87%|████████▋ | 87/100 [06:26<00:57,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 -- Loss: 0.5709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  88%|████████▊ | 88/100 [06:30<00:52,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 -- Loss: 0.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  89%|████████▉ | 89/100 [06:35<00:48,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 -- Loss: 0.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  90%|█████████ | 90/100 [06:39<00:44,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 -- Loss: 0.5703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  91%|█████████ | 91/100 [06:43<00:39,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 -- Loss: 0.5703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  92%|█████████▏| 92/100 [06:48<00:35,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 -- Loss: 0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  93%|█████████▎| 93/100 [06:52<00:30,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 -- Loss: 0.5697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  94%|█████████▍| 94/100 [06:57<00:26,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 -- Loss: 0.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  95%|█████████▌| 95/100 [07:01<00:22,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 -- Loss: 0.5689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  96%|█████████▌| 96/100 [07:06<00:17,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 -- Loss: 0.5693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  97%|█████████▋| 97/100 [07:10<00:13,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 -- Loss: 0.5690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  98%|█████████▊| 98/100 [07:15<00:08,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 -- Loss: 0.5691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  99%|█████████▉| 99/100 [07:19<00:04,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 -- Loss: 0.5684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 100/100 [07:24<00:00,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 -- Loss: 0.5684\n",
      "Ternary Classification using Self-trained embeddings Accuracy: 0.7173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the MLP for ternary classification (self-trained embeddings)\n",
    "model_tern_self = MLP(input_dim=300, hidden1=50, hidden2=10, output_dim=3)\n",
    "acc_tern_self = train_model(model_tern_self, X_train_tern_self, y_train_tern, X_test_tern_self, y_test_tern, epochs=100, lr=0.001)\n",
    "print(\"Ternary Classification using Self-trained embeddings Accuracy: {:.4f}\".format(acc_tern_self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating 10 vecters gave better output than the average vector.\n",
    "# we can see a significant gap between the binary and ternary vectors as the binary classification performs well \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating review embeddings: 100%|██████████| 2000/2000 [00:00<00:00, 9384.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 complete. Loss: 0.6665\n",
      "Epoch 2/10 complete. Loss: 0.5176\n",
      "Epoch 3/10 complete. Loss: 0.4361\n",
      "Epoch 4/10 complete. Loss: 0.3752\n",
      "Epoch 5/10 complete. Loss: 0.2762\n",
      "Epoch 6/10 complete. Loss: 0.2116\n",
      "Epoch 7/10 complete. Loss: 0.1438\n",
      "Epoch 8/10 complete. Loss: 0.1038\n",
      "Epoch 9/10 complete. Loss: 0.0604\n",
      "Epoch 10/10 complete. Loss: 0.0429\n",
      "CNN Accuracy (Binary Classification): 0.7900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- 1. Function to obtain a fixed-length review embedding matrix -----\n",
    "def get_review_embedding(review, model, max_length=50):\n",
    "    \"\"\"\n",
    "    Tokenize the review using simple_preprocess.\n",
    "    For each token found in the Word2Vec model, get its 300-dim vector.\n",
    "    Truncate if the review is longer than max_length tokens; pad with zero vectors if shorter.\n",
    "    \n",
    "    Returns a matrix of shape (max_length, 300).\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(str(review))\n",
    "    lookup = model.wv if hasattr(model, 'wv') else model\n",
    "    embeddings = []\n",
    "    for token in tokens[:max_length]:\n",
    "        if token in lookup.key_to_index:\n",
    "            embeddings.append(lookup[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(lookup.vector_size))\n",
    "    # Pad with zeros if needed.\n",
    "    while len(embeddings) < max_length:\n",
    "        embeddings.append(np.zeros(lookup.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ----- 2. Prepare the binary dataset (only sentiments 1 and 2) -----\n",
    "# Use your existing DataFrame (balanced_data) which has 'review_body' and 'sentiment'\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "# Map sentiment 1 -> 0 and sentiment 2 -> 1\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 0 if x == 1 else 1)\n",
    "# (Optional) To reduce memory load during development, limit the dataset to a smaller sample:\n",
    "binary_data = binary_data.sample(n=2000, random_state=42)\n",
    "\n",
    "# ----- 3. Load the pretrained Google News Word2Vec Model -----\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# ----- 4. Generate the review embedding matrices for each review -----\n",
    "# Each review will be represented as a (50,300) matrix.\n",
    "features = []\n",
    "for review in tqdm(binary_data['review_body'], desc=\"Generating review embeddings\"):\n",
    "    features.append(get_review_embedding(review, google_model, max_length=50))\n",
    "features = np.array(features)  # shape: (n_samples, 50, 300)\n",
    "labels = binary_data['label'].values\n",
    "\n",
    "# ----- 5. Split into Training and Testing Sets -----\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors.\n",
    "# For CNN, we want input shape: (batch, channels, height, width)\n",
    "# Here, height=max_length (50 tokens) and width=300 (embedding size). We set channel=1.\n",
    "X_train = torch.FloatTensor(X_train)  # shape: (N,50,300)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Add channel dimension: convert (N,50,300) -> (N,1,50,300)\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "\n",
    "# ----- 6. Define a Simple CNN Model -----\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # First conv: use a kernel that spans the entire embedding dimension.\n",
    "        # Input shape: (batch, 1, 50, 300) -> Output: (batch, 50, 48, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=(3, 300))\n",
    "        # Second conv: will work on the sequence dimension.\n",
    "        # Input shape: (batch, 50, 48, 1) -> Output: (batch, 10, 46, 1) if kernel_size=(3,1) and out_channels=10.\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=10, kernel_size=(3, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        # Calculate the flattened feature size: after conv2, height = 48-3+1 = 46 and width = 1.\n",
    "        self.fc = nn.Linear(10 * 46, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # (batch,50,48,1)\n",
    "        x = self.relu(self.conv2(x))  # (batch,10,46,1)\n",
    "        x = x.view(x.size(0), -1)       # flatten: (batch, 10*46)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ----- 7. Training Function using mini-batching on CUDA if available -----\n",
    "def train_cnn(model, X_train, y_train, X_test, y_test, epochs=10, lr=0.001, batch_size=16):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} complete. Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return test_acc\n",
    "\n",
    "# ----- 8. Instantiate, Train, and Evaluate the CNN Model -----\n",
    "cnn_model = TextCNN(num_classes=2)\n",
    "accuracy_cnn = train_cnn(cnn_model, X_train, y_train, X_test, y_test, epochs=10, lr=0.001, batch_size=16)\n",
    "print(\"CNN Accuracy (Binary Classification): {:.4f}\".format(accuracy_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding reviews: 100%|██████████| 2000/2000 [00:00<00:00, 8717.12it/s]\n",
      "Embedding reviews: 100%|██████████| 2000/2000 [00:00<00:00, 10572.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -- Loss: 0.6498\n",
      "Epoch 2/10 -- Loss: 0.5380\n",
      "Epoch 3/10 -- Loss: 0.4480\n",
      "Epoch 4/10 -- Loss: 0.3896\n",
      "Epoch 5/10 -- Loss: 0.3277\n",
      "Epoch 6/10 -- Loss: 0.2472\n",
      "Epoch 7/10 -- Loss: 0.1773\n",
      "Epoch 8/10 -- Loss: 0.1120\n",
      "Epoch 9/10 -- Loss: 0.0777\n",
      "Epoch 10/10 -- Loss: 0.0365\n",
      "CNN Accuracy (Pre-trained): 0.8000\n",
      "Epoch 1/10 -- Loss: 0.6444\n",
      "Epoch 2/10 -- Loss: 0.4004\n",
      "Epoch 3/10 -- Loss: 0.2281\n",
      "Epoch 4/10 -- Loss: 0.1000\n",
      "Epoch 5/10 -- Loss: 0.0333\n",
      "Epoch 6/10 -- Loss: 0.0151\n",
      "Epoch 7/10 -- Loss: 0.0080\n",
      "Epoch 8/10 -- Loss: 0.0059\n",
      "Epoch 9/10 -- Loss: 0.0049\n",
      "Epoch 10/10 -- Loss: 0.0041\n",
      "CNN Accuracy (Self-trained): 0.7875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#############################################\n",
    "# 1. Helper function: Fixed-length Review Embed.\n",
    "#############################################\n",
    "def get_review_embedding(review, model, max_length=50):\n",
    "    \"\"\"\n",
    "    Tokenizes the review using gensim's simple_preprocess.\n",
    "    For each token (up to max_length), retrieves its 300-dim vector\n",
    "    from the given Word2Vec key–vector mapping.\n",
    "    \n",
    "    If the token is not found, a 300-dim zero vector is used.\n",
    "    The result is a (max_length, 300) NumPy array.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(str(review))\n",
    "    # If the model has attribute 'wv' use that (for a full Word2Vec model),\n",
    "    # otherwise use the model directly (if it is a KeyedVectors object).\n",
    "    lookup = model.wv if hasattr(model, 'wv') else model\n",
    "    embeddings = []\n",
    "    for token in tokens[:max_length]:\n",
    "        if token in lookup.key_to_index:\n",
    "            embeddings.append(lookup[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(lookup.vector_size))\n",
    "    # Pad reviews shorter than max_length with zero vectors.\n",
    "    while len(embeddings) < max_length:\n",
    "        embeddings.append(np.zeros(lookup.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "#############################################\n",
    "# 2. Prepare Binary Dataset (Sentiments 1 and 2)\n",
    "#############################################\n",
    "binary_data = balanced_data[balanced_data['sentiment'].isin([1, 2])].copy()\n",
    "# Map sentiment 1 -> label 0 and sentiment 2 -> label 1.\n",
    "binary_data['label'] = binary_data['sentiment'].apply(lambda x: 0 if x == 1 else 1)\n",
    "# (Optional) To reduce memory usage during testing, sample a subset:\n",
    "binary_data = binary_data.sample(n=2000, random_state=42)\n",
    "\n",
    "#############################################\n",
    "# 3. Load Word2Vec Models\n",
    "#############################################\n",
    "# Pre-trained model: Google News vectors.\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "# Self-trained model: assume it’s already built and available as w2v_model.\n",
    "# (If w2v_model was saved using KeyedVectors, then it does not have a .wv attribute.)\n",
    "\n",
    "#############################################\n",
    "# 4. Generate Review Embeddings for each review.\n",
    "#############################################\n",
    "# Each review is represented as a matrix of shape (50, 300)\n",
    "def generate_review_embeddings(df, model, max_length=50):\n",
    "    features = []\n",
    "    for review in tqdm(df['review_body'], desc=\"Embedding reviews\"):\n",
    "        features.append(get_review_embedding(review, model, max_length))\n",
    "    return np.array(features)\n",
    "\n",
    "# Create features using the pre-trained model:\n",
    "X_pre = generate_review_embeddings(binary_data, google_model, max_length=50)\n",
    "# Create features using the self-trained model:\n",
    "X_self = generate_review_embeddings(binary_data, w2v_model, max_length=50)\n",
    "y = binary_data['label'].values\n",
    "\n",
    "#############################################\n",
    "# 5. Split the Data (80% Train, 20% Test)\n",
    "#############################################\n",
    "X_train_pre, X_test_pre, y_train, y_test = train_test_split(X_pre, y, test_size=0.2, random_state=42)\n",
    "X_train_self, X_test_self, _, _ = train_test_split(X_self, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#############################################\n",
    "# 6. Convert Data to PyTorch Tensors and Reshape\n",
    "#############################################\n",
    "# For a CNN, input shape should be (N, channels, height, width).\n",
    "# Here, height = max_length (50 tokens) and width = 300 (embedding size); we set channels = 1.\n",
    "def to_tensor(X, y=None):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    if y is not None:\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        return X_tensor, y_tensor\n",
    "    return X_tensor\n",
    "\n",
    "X_train_pre, y_train = to_tensor(X_train_pre, y_train)\n",
    "X_test_pre, y_test   = to_tensor(X_test_pre, y_test)\n",
    "X_train_self = to_tensor(X_train_self)\n",
    "X_test_self  = to_tensor(X_test_self)\n",
    "\n",
    "# Add a channel dimension: (N, 50, 300) -> (N, 1, 50, 300)\n",
    "X_train_pre = X_train_pre.unsqueeze(1)\n",
    "X_test_pre  = X_test_pre.unsqueeze(1)\n",
    "X_train_self = X_train_self.unsqueeze(1)\n",
    "X_test_self  = X_test_self.unsqueeze(1)\n",
    "\n",
    "#############################################\n",
    "# 7. Define a Simple CNN for Sentiment Classification\n",
    "#############################################\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # First convolution: kernel size covers 3 tokens and entire embedding.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=(3, 300))\n",
    "        # Second convolution: further extract features from the sequence dimension.\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=10, kernel_size=(3, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        # Calculate flattened feature size:\n",
    "        # After conv1: if input height=50, kernel_height=3, output height = 50-3+1 = 48.\n",
    "        # After conv2: output height = 48-3+1 = 46, width remains 1.\n",
    "        self.fc = nn.Linear(10 * 46, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # shape: (N, 50, 48, 1)\n",
    "        x = self.relu(self.conv2(x))  # shape: (N, 10, 46, 1)\n",
    "        x = x.view(x.size(0), -1)      # flatten to (N, 10*46)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 8. Define the Training Procedure Using Mini-Batches and CUDA\n",
    "#############################################\n",
    "def train_cnn(model, X_train, y_train, X_test, y_test, epochs=10, lr=0.001, batch_size=16):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return test_acc\n",
    "\n",
    "#############################################\n",
    "# 9. Train and Evaluate the CNN Models\n",
    "#############################################\n",
    "# (A) Using Pre-trained Embeddings:\n",
    "cnn_model_pre = TextCNN(num_classes=2)\n",
    "acc_cnn_pre = train_cnn(cnn_model_pre, X_train_pre, y_train, X_test_pre, y_test,\n",
    "                        epochs=10, lr=0.001, batch_size=16)\n",
    "print(\"CNN Accuracy (Pre-trained): {:.4f}\".format(acc_cnn_pre))\n",
    "\n",
    "# (B) Using Self-trained Embeddings:\n",
    "cnn_model_self = TextCNN(num_classes=2)\n",
    "acc_cnn_self = train_cnn(cnn_model_self, X_train_self, y_train, X_test_self, y_test,\n",
    "                         epochs=10, lr=0.001, batch_size=16)\n",
    "print(\"CNN Accuracy (Self-trained): {:.4f}\".format(acc_cnn_self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating review embeddings: 100%|██████████| 2000/2000 [00:00<00:00, 6932.16it/s]\n",
      "Generating review embeddings: 100%|██████████| 2000/2000 [00:00<00:00, 7359.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 -- Loss: 1.0929\n",
      "Epoch 2/100 -- Loss: 1.0856\n",
      "Epoch 3/100 -- Loss: 1.0774\n",
      "Epoch 4/100 -- Loss: 1.0686\n",
      "Epoch 5/100 -- Loss: 1.0596\n",
      "Epoch 6/100 -- Loss: 1.0518\n",
      "Epoch 7/100 -- Loss: 1.0460\n",
      "Epoch 8/100 -- Loss: 1.0419\n",
      "Epoch 9/100 -- Loss: 1.0390\n",
      "Epoch 10/100 -- Loss: 1.0368\n",
      "Epoch 11/100 -- Loss: 1.0347\n",
      "Epoch 12/100 -- Loss: 1.0328\n",
      "Epoch 13/100 -- Loss: 1.0311\n",
      "Epoch 14/100 -- Loss: 1.0291\n",
      "Epoch 15/100 -- Loss: 1.0272\n",
      "Epoch 16/100 -- Loss: 1.0252\n",
      "Epoch 17/100 -- Loss: 1.0232\n",
      "Epoch 18/100 -- Loss: 1.0209\n",
      "Epoch 19/100 -- Loss: 1.0187\n",
      "Epoch 20/100 -- Loss: 1.0163\n",
      "Epoch 21/100 -- Loss: 1.0139\n",
      "Epoch 22/100 -- Loss: 1.0112\n",
      "Epoch 23/100 -- Loss: 1.0087\n",
      "Epoch 24/100 -- Loss: 1.0058\n",
      "Epoch 25/100 -- Loss: 1.0029\n",
      "Epoch 26/100 -- Loss: 0.9999\n",
      "Epoch 27/100 -- Loss: 0.9969\n",
      "Epoch 28/100 -- Loss: 0.9938\n",
      "Epoch 29/100 -- Loss: 0.9905\n",
      "Epoch 30/100 -- Loss: 0.9871\n",
      "Epoch 31/100 -- Loss: 0.9836\n",
      "Epoch 32/100 -- Loss: 0.9800\n",
      "Epoch 33/100 -- Loss: 0.9766\n",
      "Epoch 34/100 -- Loss: 0.9727\n",
      "Epoch 35/100 -- Loss: 0.9692\n",
      "Epoch 36/100 -- Loss: 0.9653\n",
      "Epoch 37/100 -- Loss: 0.9615\n",
      "Epoch 38/100 -- Loss: 0.9577\n",
      "Epoch 39/100 -- Loss: 0.9538\n",
      "Epoch 40/100 -- Loss: 0.9500\n",
      "Epoch 41/100 -- Loss: 0.9459\n",
      "Epoch 42/100 -- Loss: 0.9420\n",
      "Epoch 43/100 -- Loss: 0.9381\n",
      "Epoch 44/100 -- Loss: 0.9342\n",
      "Epoch 45/100 -- Loss: 0.9302\n",
      "Epoch 46/100 -- Loss: 0.9263\n",
      "Epoch 47/100 -- Loss: 0.9226\n",
      "Epoch 48/100 -- Loss: 0.9187\n",
      "Epoch 49/100 -- Loss: 0.9149\n",
      "Epoch 50/100 -- Loss: 0.9113\n",
      "Epoch 51/100 -- Loss: 0.9075\n",
      "Epoch 52/100 -- Loss: 0.9039\n",
      "Epoch 53/100 -- Loss: 0.9002\n",
      "Epoch 54/100 -- Loss: 0.8967\n",
      "Epoch 55/100 -- Loss: 0.8930\n",
      "Epoch 56/100 -- Loss: 0.8902\n",
      "Epoch 57/100 -- Loss: 0.8864\n",
      "Epoch 58/100 -- Loss: 0.8831\n",
      "Epoch 59/100 -- Loss: 0.8797\n",
      "Epoch 60/100 -- Loss: 0.8768\n",
      "Epoch 61/100 -- Loss: 0.8733\n",
      "Epoch 62/100 -- Loss: 0.8705\n",
      "Epoch 63/100 -- Loss: 0.8672\n",
      "Epoch 64/100 -- Loss: 0.8642\n",
      "Epoch 65/100 -- Loss: 0.8614\n",
      "Epoch 66/100 -- Loss: 0.8583\n",
      "Epoch 67/100 -- Loss: 0.8553\n",
      "Epoch 68/100 -- Loss: 0.8527\n",
      "Epoch 69/100 -- Loss: 0.8498\n",
      "Epoch 70/100 -- Loss: 0.8473\n",
      "Epoch 71/100 -- Loss: 0.8447\n",
      "Epoch 72/100 -- Loss: 0.8419\n",
      "Epoch 73/100 -- Loss: 0.8392\n",
      "Epoch 74/100 -- Loss: 0.8366\n",
      "Epoch 75/100 -- Loss: 0.8343\n",
      "Epoch 76/100 -- Loss: 0.8316\n",
      "Epoch 77/100 -- Loss: 0.8291\n",
      "Epoch 78/100 -- Loss: 0.8269\n",
      "Epoch 79/100 -- Loss: 0.8246\n",
      "Epoch 80/100 -- Loss: 0.8220\n",
      "Epoch 81/100 -- Loss: 0.8195\n",
      "Epoch 82/100 -- Loss: 0.8181\n",
      "Epoch 83/100 -- Loss: 0.8156\n",
      "Epoch 84/100 -- Loss: 0.8134\n",
      "Epoch 85/100 -- Loss: 0.8109\n",
      "Epoch 86/100 -- Loss: 0.8091\n",
      "Epoch 87/100 -- Loss: 0.8067\n",
      "Epoch 88/100 -- Loss: 0.8046\n",
      "Epoch 89/100 -- Loss: 0.8026\n",
      "Epoch 90/100 -- Loss: 0.8007\n",
      "Epoch 91/100 -- Loss: 0.7984\n",
      "Epoch 92/100 -- Loss: 0.7966\n",
      "Epoch 93/100 -- Loss: 0.7944\n",
      "Epoch 94/100 -- Loss: 0.7924\n",
      "Epoch 95/100 -- Loss: 0.7905\n",
      "Epoch 96/100 -- Loss: 0.7884\n",
      "Epoch 97/100 -- Loss: 0.7867\n",
      "Epoch 98/100 -- Loss: 0.7848\n",
      "Epoch 99/100 -- Loss: 0.7828\n",
      "Epoch 100/100 -- Loss: 0.7818\n",
      "Ternary Classification using Pre-trained Embeddings Accuracy: 0.5675\n",
      "Epoch 1/100 -- Loss: 1.0950\n",
      "Epoch 2/100 -- Loss: 1.0829\n",
      "Epoch 3/100 -- Loss: 1.0710\n",
      "Epoch 4/100 -- Loss: 1.0593\n",
      "Epoch 5/100 -- Loss: 1.0483\n",
      "Epoch 6/100 -- Loss: 1.0382\n",
      "Epoch 7/100 -- Loss: 1.0294\n",
      "Epoch 8/100 -- Loss: 1.0219\n",
      "Epoch 9/100 -- Loss: 1.0154\n",
      "Epoch 10/100 -- Loss: 1.0096\n",
      "Epoch 11/100 -- Loss: 1.0041\n",
      "Epoch 12/100 -- Loss: 0.9988\n",
      "Epoch 13/100 -- Loss: 0.9936\n",
      "Epoch 14/100 -- Loss: 0.9883\n",
      "Epoch 15/100 -- Loss: 0.9831\n",
      "Epoch 16/100 -- Loss: 0.9781\n",
      "Epoch 17/100 -- Loss: 0.9729\n",
      "Epoch 18/100 -- Loss: 0.9677\n",
      "Epoch 19/100 -- Loss: 0.9623\n",
      "Epoch 20/100 -- Loss: 0.9570\n",
      "Epoch 21/100 -- Loss: 0.9513\n",
      "Epoch 22/100 -- Loss: 0.9457\n",
      "Epoch 23/100 -- Loss: 0.9401\n",
      "Epoch 24/100 -- Loss: 0.9343\n",
      "Epoch 25/100 -- Loss: 0.9283\n",
      "Epoch 26/100 -- Loss: 0.9223\n",
      "Epoch 27/100 -- Loss: 0.9163\n",
      "Epoch 28/100 -- Loss: 0.9098\n",
      "Epoch 29/100 -- Loss: 0.9035\n",
      "Epoch 30/100 -- Loss: 0.8972\n",
      "Epoch 31/100 -- Loss: 0.8906\n",
      "Epoch 32/100 -- Loss: 0.8841\n",
      "Epoch 33/100 -- Loss: 0.8774\n",
      "Epoch 34/100 -- Loss: 0.8708\n",
      "Epoch 35/100 -- Loss: 0.8641\n",
      "Epoch 36/100 -- Loss: 0.8574\n",
      "Epoch 37/100 -- Loss: 0.8507\n",
      "Epoch 38/100 -- Loss: 0.8439\n",
      "Epoch 39/100 -- Loss: 0.8372\n",
      "Epoch 40/100 -- Loss: 0.8304\n",
      "Epoch 41/100 -- Loss: 0.8238\n",
      "Epoch 42/100 -- Loss: 0.8172\n",
      "Epoch 43/100 -- Loss: 0.8107\n",
      "Epoch 44/100 -- Loss: 0.8038\n",
      "Epoch 45/100 -- Loss: 0.7976\n",
      "Epoch 46/100 -- Loss: 0.7912\n",
      "Epoch 47/100 -- Loss: 0.7849\n",
      "Epoch 48/100 -- Loss: 0.7784\n",
      "Epoch 49/100 -- Loss: 0.7722\n",
      "Epoch 50/100 -- Loss: 0.7661\n",
      "Epoch 51/100 -- Loss: 0.7600\n",
      "Epoch 52/100 -- Loss: 0.7541\n",
      "Epoch 53/100 -- Loss: 0.7480\n",
      "Epoch 54/100 -- Loss: 0.7421\n",
      "Epoch 55/100 -- Loss: 0.7365\n",
      "Epoch 56/100 -- Loss: 0.7307\n",
      "Epoch 57/100 -- Loss: 0.7248\n",
      "Epoch 58/100 -- Loss: 0.7193\n",
      "Epoch 59/100 -- Loss: 0.7134\n",
      "Epoch 60/100 -- Loss: 0.7081\n",
      "Epoch 61/100 -- Loss: 0.7026\n",
      "Epoch 62/100 -- Loss: 0.6972\n",
      "Epoch 63/100 -- Loss: 0.6918\n",
      "Epoch 64/100 -- Loss: 0.6868\n",
      "Epoch 65/100 -- Loss: 0.6815\n",
      "Epoch 66/100 -- Loss: 0.6764\n",
      "Epoch 67/100 -- Loss: 0.6710\n",
      "Epoch 68/100 -- Loss: 0.6660\n",
      "Epoch 69/100 -- Loss: 0.6608\n",
      "Epoch 70/100 -- Loss: 0.6559\n",
      "Epoch 71/100 -- Loss: 0.6513\n",
      "Epoch 72/100 -- Loss: 0.6464\n",
      "Epoch 73/100 -- Loss: 0.6415\n",
      "Epoch 74/100 -- Loss: 0.6367\n",
      "Epoch 75/100 -- Loss: 0.6319\n",
      "Epoch 76/100 -- Loss: 0.6272\n",
      "Epoch 77/100 -- Loss: 0.6225\n",
      "Epoch 78/100 -- Loss: 0.6182\n",
      "Epoch 79/100 -- Loss: 0.6134\n",
      "Epoch 80/100 -- Loss: 0.6087\n",
      "Epoch 81/100 -- Loss: 0.6042\n",
      "Epoch 82/100 -- Loss: 0.5996\n",
      "Epoch 83/100 -- Loss: 0.5952\n",
      "Epoch 84/100 -- Loss: 0.5910\n",
      "Epoch 85/100 -- Loss: 0.5865\n",
      "Epoch 86/100 -- Loss: 0.5823\n",
      "Epoch 87/100 -- Loss: 0.5775\n",
      "Epoch 88/100 -- Loss: 0.5732\n",
      "Epoch 89/100 -- Loss: 0.5690\n",
      "Epoch 90/100 -- Loss: 0.5647\n",
      "Epoch 91/100 -- Loss: 0.5605\n",
      "Epoch 92/100 -- Loss: 0.5564\n",
      "Epoch 93/100 -- Loss: 0.5519\n",
      "Epoch 94/100 -- Loss: 0.5478\n",
      "Epoch 95/100 -- Loss: 0.5437\n",
      "Epoch 96/100 -- Loss: 0.5403\n",
      "Epoch 97/100 -- Loss: 0.5359\n",
      "Epoch 98/100 -- Loss: 0.5316\n",
      "Epoch 99/100 -- Loss: 0.5275\n",
      "Epoch 100/100 -- Loss: 0.5238\n",
      "Ternary Classification using Self-trained Embeddings Accuracy: 0.5950\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#############################################\n",
    "# 1. Helper function to generate a fixed-length review embedding matrix\n",
    "#############################################\n",
    "def get_review_embedding(review, model, max_length=50):\n",
    "    \"\"\"\n",
    "    Tokenizes the review using gensim's simple_preprocess.\n",
    "    Retrieves a 300-dimensional vector for each token from the given model.\n",
    "    If the token is not found, uses a zero vector.\n",
    "    Truncates to max_length tokens or pads with zeros if needed.\n",
    "    Returns a numpy array of shape (max_length, 300).\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(str(review))\n",
    "    # Use model.wv if available (for a full Word2Vec model), otherwise use model directly.\n",
    "    lookup = model.wv if hasattr(model, 'wv') else model\n",
    "    embeddings = []\n",
    "    for token in tokens[:max_length]:\n",
    "        if token in lookup.key_to_index:\n",
    "            embeddings.append(lookup[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(lookup.vector_size))\n",
    "    while len(embeddings) < max_length:\n",
    "        embeddings.append(np.zeros(lookup.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "#############################################\n",
    "# 2. Prepare the Ternary Dataset\n",
    "#############################################\n",
    "# Map sentiment 1,2,3 to labels 0,1,2.\n",
    "ternary_data = balanced_data.copy()\n",
    "ternary_data['label'] = ternary_data['sentiment'] - 1\n",
    "# To limit resource usage, sample a subset (adjust n as needed).\n",
    "ternary_data = ternary_data.sample(n=2000, random_state=42)\n",
    "\n",
    "#############################################\n",
    "# 3. Load Word2Vec Models\n",
    "#############################################\n",
    "# Pre-trained model (Google News):\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "# Self-trained model is assumed available as w2v_model.\n",
    "\n",
    "#############################################\n",
    "# 4. Generate fixed-length review embeddings for each review\n",
    "#############################################\n",
    "def generate_review_embeddings(df, model, max_length=50):\n",
    "    features = []\n",
    "    for review in tqdm(df['review_body'], desc=\"Generating review embeddings\"):\n",
    "        features.append(get_review_embedding(review, model, max_length))\n",
    "    return np.array(features)\n",
    "\n",
    "# Generate features for both variants:\n",
    "X_tern_pre = generate_review_embeddings(ternary_data, google_model, max_length=50)\n",
    "X_tern_self = generate_review_embeddings(ternary_data, w2v_model, max_length=50)\n",
    "y_tern = ternary_data['label'].values\n",
    "\n",
    "#############################################\n",
    "# 5. Split Data into Training and Testing Sets (80/20 Split)\n",
    "#############################################\n",
    "X_train_pre, X_test_pre, y_train, y_test = train_test_split(X_tern_pre, y_tern, test_size=0.2, random_state=3)\n",
    "X_train_self, X_test_self, _, _ = train_test_split(X_tern_self, y_tern, test_size=0.2, random_state=3)\n",
    "\n",
    "#############################################\n",
    "# 6. Convert to PyTorch Tensors and reshape for CNN input\n",
    "#############################################\n",
    "def to_tensor(X, y=None):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    if y is not None:\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "    else:\n",
    "        y_tensor = None\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "X_train_pre, y_train = to_tensor(X_train_pre, y_train)\n",
    "X_test_pre, y_test   = to_tensor(X_test_pre, y_test)\n",
    "X_train_self, _ = to_tensor(X_train_self, None)\n",
    "X_test_self, _  = to_tensor(X_test_self, None)\n",
    "\n",
    "# For CNN, add a channel dimension:\n",
    "# Current shape: (N, 50, 300) -> Desired shape: (N, 1, 50, 300)\n",
    "X_train_pre = X_train_pre.unsqueeze(1)\n",
    "X_test_pre  = X_test_pre.unsqueeze(1)\n",
    "X_train_self = X_train_self.unsqueeze(1)\n",
    "X_test_self  = X_test_self.unsqueeze(1)\n",
    "\n",
    "#############################################\n",
    "# 7. Define the CNN Model Architecture for Ternary Classification\n",
    "#############################################\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # First convolution: kernel spans 3 tokens and entire embedding dimension.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=(3, 300))\n",
    "        # Second convolution: further processes the sequence (height) dimension.\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=10, kernel_size=(3, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        # After conv1: height = 50-3+1 = 48.\n",
    "        # After conv2: height = 48-3+1 = 46; width remains 1.\n",
    "        self.fc = nn.Linear(10 * 46, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # (N, 50, 48, 1)\n",
    "        x = self.relu(self.conv2(x))  # (N, 10, 46, 1)\n",
    "        x = x.view(x.size(0), -1)       # Flatten to (N, 10*46)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#############################################\n",
    "# 8. Define the Training Function (with mini-batching and CUDA)\n",
    "#############################################\n",
    "def train_cnn(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.00001, batch_size=8):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test  = X_test.to(device)\n",
    "    y_test  = y_test.to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "    return test_acc\n",
    "\n",
    "#############################################\n",
    "# 9. Train and Evaluate the CNN for Ternary Classification\n",
    "#############################################\n",
    "# (A) Using Pre-trained Embeddings:\n",
    "cnn_model_tern_pre = TextCNN(num_classes=3)\n",
    "acc_tern_pre = train_cnn(cnn_model_tern_pre, X_train_pre, y_train, X_test_pre, y_test, epochs=100, lr=0.00001, batch_size=8)\n",
    "print(\"Ternary Classification using Pre-trained Embeddings Accuracy: {:.4f}\".format(acc_tern_pre))\n",
    "\n",
    "# (B) Using Self-trained Embeddings:\n",
    "cnn_model_tern_self = TextCNN(num_classes=3)\n",
    "acc_tern_self = train_cnn(cnn_model_tern_self, X_train_self, y_train, X_test_self, y_test, epochs=100, lr=0.00001, batch_size=8)\n",
    "print(\"Ternary Classification using Self-trained Embeddings Accuracy: {:.4f}\".format(acc_tern_self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best accuracy for the above code was obtained when I computed on CARC A100 gpus but they were not available during final execution, so here is the best i could get for the above code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
